{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Anomaly Detection Data Preprocessing\n",
    "## 0. Prerequisites\n",
    "### Operating System\n",
    "If you are using Mac OS, Linux, or WSL on Windows, skip to **Python Package Dependencies**.\n",
    "\n",
    "Otherwise, follow the steps in [Anaconda_setup.ipynb](https://github.com/451488975/Anaconda_Setup) to set up WSL on Windows. Since WSL currently does not have GPU support, follow the steps in the tutorial labelled with \"w/o NVIDIA GPU.\"\n",
    "### Python Package Dependencies\n",
    "Install the following dependencies:\n",
    "* numpy\n",
    "* matplotlib\n",
    "* pandas\n",
    "* tqdm\n",
    "* h5py\n",
    "* pyjet (not supported on Windows)\n",
    "\n",
    "If you have Anaconda, first try installing them via Anaconda (make sure you're in the right environment!) by running `conda install [package name]` in terminal. If the package is not found by Anaconda, install with pip by running `pip install [package name]` in terminal.\n",
    "### Data\n",
    "We will be working with the [LHCO2020 R&D Dataset](https://zenodo.org/record/3832254#.X9VHi9hKguU). Download `events_anomalydetection.h5` from the link.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyjet'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4bdc19d7879c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyjet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDTYPE_PTEPM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyjet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtestdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_event\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyjet'"
     ]
    }
   ],
   "source": [
    "# Load all our modules\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pyjet import cluster, DTYPE_PTEPM\n",
    "from pyjet.testdata import get_event"
   ]
  },
  {
   "source": [
    "## 1. Loading Data\n",
    "### Reading Data\n",
    "Our data is stored as pandas dataframes in the compressed `.h5` file format. Since it's a large dataset, we can't read it in its entirety because it will likely exceed our RAM capacity. For the purposes of feature study, we do not need to read the entire dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape: (100000, 2101)\n",
      "Memory in GB: 1.5661120414733887\n"
     ]
    }
   ],
   "source": [
    "# Replace this variable with your relative path to datafile\n",
    "datapath = \"data/events_anomalydetection.h5\"\n",
    "\n",
    "df = pd.read_hdf(datapath, stop = 100000) # The amount of data loaded can be reduced by an order of magnitude on older hardware\n",
    "print(\"Shape: \" + str(df.shape))\n",
    "print(\"Memory usage in GB:\", round(sum(df.memory_usage(deep=True)) / (1024**3), 3))"
   ]
  },
  {
   "source": [
    "### Understanding Data\n",
    "The data is described on the [Zenodo site](https://zenodo.org/record/3832254#.X9VHi9hKguU). In essence, each row corresponds to an event, which is either signal (W'->X+Y) or background QCD dijet.\n",
    "\n",
    "The shape of the original dataset is `(1100000,2101)`. There are 1M background rows and 100k signal rows. The last column contains a single bit that indicates (for the purpose of R&D) `0` if the row is background and `1` if the row is signal.\n",
    "\n",
    "Each row consists of up to 700 massless final-state, charged, hadronic particles recorded in `(pT,eta,phi)` triplets. Most rows don't have 700 non-trivial particles, and have instead been zero-padded up to 700 triplets.\n",
    "\n",
    "The signal event consists of a hypothetical hadron W' with rest mass 3.5 TeV decaying into hypothetical hadrons X and Y with rest masses 500 GeV and 100 GeV, respectively. We expect the W' to be non-relativistic in the lab frame. We expect X to have total energy 1785 GeV and Y to have total energy 1716 GeV. Optionally, you can view the [derivation](https://www.overleaf.com/read/mpmgzzdcsyxz\n",
    ").\n",
    "\n",
    "X and Y then decay into two quarks each: X->qq, Y->qq. Since X and Y are relativistic, the quarks will be part of the same jet; that is, the signal event consists of two jets, one associated with X and one associated with Y. Each one of those jets will have two-pronged substructure, which we can see later by studying the tau2/tau1 feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the following about the data:\n",
    "# pT columns are always positive\n",
    "# Zero-padding is evident at the end\n",
    "# Last column (index 2100) is the signal bit\n",
    "# Signal and background are randomly shuffled within the data\n",
    "df"
   ]
  },
  {
   "source": [
    "# Split the data based on the signal bit and discard the flag column\n",
    "df_bg = df[df[2100] == 0.0].iloc[:,:-1]\n",
    "df_s = df[df[2100] == 1.0].iloc[:,:-1]\n",
    "df = df.iloc[:,:-1]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Background shape: \" + str(df_bg.shape))\n",
    "print(\"Signal shape: \" + str(df_s.shape))\n",
    "print(\"Percent of signal in data: \" + str(round(df_s.shape[0] / df.shape[0] * 100, 2)) + \"%\")"
   ]
  },
  {
   "source": [
    "## 2. Cluster Jets\n",
    "The goal is to cluster each row into two jets to reflect the W'->X,Y structure of the signal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 3. Plot Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. Exercise\n",
    "Now it's your turn! Do the data preprocessing and feature study, as we've done in this tutorial, using the file `events_anomalydetection_Z_XY_qqq.h5`, which you should download at the same [Zenodo link](https://zenodo.org/record/3832254#.X9VNZdhKguU). Note a few key differences:\n",
    "* This file only has 100k signal events and no background\n",
    "* Therefore, there is no signal bit column\n",
    "* The signal has 3-prong substructure, so the proper N-subjetiness ratio to compute is tau3/tau2 instead of tau2/tau1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}