{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Wasserstein GAN\n",
    "## For generation of features based on LHCO2020 datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0\nPhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras.backend as K\n",
    "import gc\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    \"herwig\": \"GAN-data\\events_anomalydetection_DelphesHerwig_qcd_features.h5\",\n",
    "    \"pythiabg\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_qcd_features.h5\",\n",
    "    \"pythiasig\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_Wprime_features.h5\"\n",
    "}\n",
    "\n",
    "datatypes = [\"herwig\", \"pythiabg\", \"pythiasig\"]\n",
    "features = [\"px\", \"py\", \"pz\", \"m\", \"tau1\", \"tau2\", \"tau3\"]\n",
    "\n",
    "train_features = [\"tau21j2\"] # Can be flexibly changed to suit GAN needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datatype, stop = None):\n",
    "    input_frame = pd.read_hdf(filenames[datatype], stop = stop)\n",
    "    output_frame = input_frame.copy()\n",
    "    for feature in features:\n",
    "        output_frame[feature + \"j1\"] = (input_frame[\"mj1\"] >= input_frame[\"mj2\"])*input_frame[feature + \"j1\"] + (input_frame[\"mj1\"] < input_frame[\"mj2\"])*input_frame[feature + \"j2\"]\n",
    "        output_frame[feature + \"j2\"] = (input_frame[\"mj1\"] >= input_frame[\"mj2\"])*input_frame[feature + \"j2\"] + (input_frame[\"mj1\"] < input_frame[\"mj2\"])*input_frame[feature + \"j1\"]\n",
    "    del input_frame\n",
    "    gc.collect()\n",
    "    output_frame[\"ej1\"] = np.sqrt(output_frame[\"mj1\"]**2 + output_frame[\"pxj1\"]**2 + output_frame[\"pyj1\"]**2 + output_frame[\"pzj1\"]**2)\n",
    "    output_frame[\"ej2\"] = np.sqrt(output_frame[\"mj2\"]**2 + output_frame[\"pxj2\"]**2 + output_frame[\"pyj2\"]**2 + output_frame[\"pzj2\"]**2)\n",
    "    output_frame[\"ejj\"] = output_frame[\"ej1\"] + output_frame[\"ej2\"]\n",
    "    output_frame[\"pjj\"] = np.sqrt((output_frame[\"pxj1\"] + output_frame[\"pxj2\"])**2 + (output_frame[\"pyj1\"] + output_frame[\"pyj2\"])**2 + (output_frame[\"pyj1\"] + output_frame[\"pyj2\"])**2)\n",
    "    output_frame[\"mjj\"] = np.sqrt(output_frame[\"ejj\"]**2 - output_frame[\"pjj\"]**2)\n",
    "    output_frame[\"tau21j1\"] = output_frame[\"tau2j1\"] / output_frame[\"tau1j1\"]\n",
    "    output_frame[\"tau32j1\"] = output_frame[\"tau3j1\"] / output_frame[\"tau2j1\"]\n",
    "    output_frame[\"tau21j2\"] = output_frame[\"tau2j2\"] / output_frame[\"tau1j2\"]\n",
    "    output_frame[\"tau32j2\"] = output_frame[\"tau3j2\"] / output_frame[\"tau2j2\"]\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "\n",
    "BATCH_SIZE = 512 # Uses about 3 GB of VRAM for batch size of 512\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 0.00005\n",
    "N_CRITIC = 5\n",
    "C_LAMBDA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"herwig\")\n",
    "\n",
    "# Ensures all batches have same size\n",
    "\n",
    "df.drop([i for i in range(df.shape[0] % (BATCH_SIZE * 4))], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset consists of 2928 batches of 256 samples each, total 749568 samples\nTestset consists of 976 batches of 256 samples each, total 249856 samples\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Normalize all inputs between -1 and 1\n",
    "\n",
    "scaler = MinMaxScaler((-1,1)).fit(df[train_features])\n",
    "feature_df = scaler.transform(df[train_features])\n",
    "\n",
    "X_train, X_test = train_test_split(feature_df, test_size = 0.25)\n",
    "len_dataset = int(X_train.shape[0] / BATCH_SIZE)\n",
    "len_testset = int(X_test.shape[0] / BATCH_SIZE)\n",
    "print(\"Dataset consists of {} batches of {} samples each, total {} samples\".format(len_dataset, BATCH_SIZE, len_dataset * BATCH_SIZE))\n",
    "print(\"Testset consists of {} batches of {} samples each, total {} samples\".format(len_testset, BATCH_SIZE, len_testset * BATCH_SIZE))\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(np.array(X_train)).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(np.array(X_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(50, input_shape=(len(train_features),)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(len(train_features), activation='tanh'))\n",
    "    assert model.output_shape == (None, len(train_features))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(50, input_shape=(len(train_features),)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(1)) # WGAN: No sigmoid activation in last layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 50)                100       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 50)                200       \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                2550      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 50)                200       \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 50)                2550      \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 50)                200       \n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 5,851\nTrainable params: 5,551\nNon-trainable params: 300\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_4 (Dense)              (None, 50)                100       \n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndropout (Dropout)            (None, 50)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 50)                2550      \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 2,701\nTrainable params: 2,701\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_penalty(real, fake, epsilon): \n",
    "    # mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "    mixed_images = fake + epsilon * (real - fake)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(mixed_images) \n",
    "        mixed_scores = discriminator(mixed_images)\n",
    "        \n",
    "    gradient = tape.gradient(mixed_scores, mixed_images)[0]\n",
    "    \n",
    "    gradient_norm = tf.norm(gradient)\n",
    "    penalty = tf.math.reduce_mean((gradient_norm - 1)**2)\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def discriminator_loss(real_output, fake_output, gradient_penalty):\n",
    "    loss = tf.math.reduce_mean(fake_output) - tf.math.reduce_mean(real_output) + C_LAMBDA * gradient_penalty\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generator_loss(fake_output):\n",
    "    gen_loss = -1. * tf.math.reduce_mean(fake_output)\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.RMSprop(lr=LEARNING_RATE)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to float for loss function plotting\n",
    "def K_eval(x):\n",
    "    try:\n",
    "        return K.get_value(K.to_dense(x))\n",
    "    except:\n",
    "        eval_fn = K.function([], [x])\n",
    "        return eval_fn([])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_generator(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  \n",
    "  return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_discriminator(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "\n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    epsilon = tf.random.normal([BATCH_SIZE,len(train_features)])\n",
    "    gp = gradient_penalty(images, generated_images, epsilon)\n",
    "    \n",
    "    disc_loss = discriminator_loss(real_output, fake_output, gp)\n",
    "\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "  \n",
    "  return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_generator():\n",
    "    noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "    generated_images = generator(noise, training=False)\n",
    "\n",
    "    fake_output = discriminator(generated_images, training=False)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_discriminator(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "    generated_images = generator(noise, training=False)\n",
    "\n",
    "    real_output = discriminator(images, training=False)\n",
    "    fake_output = discriminator(generated_images, training=False)\n",
    "\n",
    "    epsilon = tf.random.normal([BATCH_SIZE,len(train_features)])\n",
    "    gp = gradient_penalty(images, generated_images, epsilon)\n",
    "    \n",
    "    disc_loss = discriminator_loss(real_output, fake_output, gp)\n",
    "\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_gan(generator):\n",
    "    fakedata = scaler.inverse_transform(generator(tf.random.normal((10000, len(train_features))), training=False))\n",
    "    plt.title(\"N-subjettiness ratio\")\n",
    "    plt.ylabel(\"Normalized to Unity\")\n",
    "    plt.xlabel(\"$\\\\tau_{21J_2}$\")\n",
    "    plt.hist(scaler.inverse_transform(X_train), bins = 25, range = (0, 1), color = \"tab:orange\", alpha = 0.5, label = \"Herwig Background\", density = True)\n",
    "    plt.hist(fakedata, bins = 25, range = (0, 1), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_losses = []\n",
    "train_disc_losses = []\n",
    "test_gen_losses = []\n",
    "test_disc_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_genloss():\n",
    "    plt.title(\"Generator Loss\")\n",
    "    plt.ylabel(\"Wasserstein Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.plot(train_gen_losses, 'b', label = \"Training loss\")\n",
    "    plt.plot(test_gen_losses, 'r', label = \"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_discloss():\n",
    "    plt.title(\"Discriminator Loss\")\n",
    "    plt.ylabel(\"Wasserstein Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.plot(train_disc_losses, 'b', label = \"Training loss\")\n",
    "    plt.plot(test_disc_losses, 'r', label = \"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, testset, epochs, n_critic):\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    print_losses = False #((epoch + 1) % 20 == 0)\n",
    "    draw_outputs = ((epoch + 1) % 10 == 0)\n",
    "\n",
    "    train_gen_loss = 0\n",
    "    train_disc_loss = 0\n",
    "\n",
    "    test_gen_loss = 0\n",
    "    test_disc_loss = 0\n",
    "\n",
    "    # Training\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_gen_loss += K_eval(train_step_generator(image_batch))\n",
    "      for n in range(n_critic):\n",
    "        train_disc_loss += K_eval(train_step_discriminator(image_batch))\n",
    "    \n",
    "    train_gen_losses.append(train_gen_loss / len_dataset)\n",
    "    train_disc_losses.append(train_disc_loss / len_dataset / n_critic)\n",
    "\n",
    "    # Evaluation\n",
    "\n",
    "    for test_batch in testset:\n",
    "      test_gen_loss += K_eval(evaluate_generator())\n",
    "      test_disc_loss += K_eval(evaluate_discriminator(test_batch))\n",
    "\n",
    "    test_gen_losses.append(test_gen_loss / len_testset)\n",
    "    test_disc_losses.append(test_disc_loss / len_testset)\n",
    "\n",
    "    # Logging\n",
    "\n",
    "    if print_losses:\n",
    "      print()\n",
    "      print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "      print()\n",
    "      print(\"Generator training loss: \" + str(train_gen_losses[-1]))\n",
    "      print(\"Discriminator training loss: \" + str(train_disc_losses[-1]))\n",
    "      print()\n",
    "      print(\"Generator validation loss: \" + str(test_gen_losses[-1]))\n",
    "      print(\"Discriminator validation loss: \" + str(test_disc_losses[-1]))\n",
    "\n",
    "    if draw_outputs:\n",
    "      print()\n",
    "      print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "      graph_gan(generator)\n",
    "      graph_genloss()\n",
    "      graph_discloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/500 [01:12<10:02:23, 72.43s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7c271f654e6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_CRITIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-be2403ae2c33>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, testset, epochs, n_critic)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m       \u001b[0mtrain_gen_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mK_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_critic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_disc_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mK_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset, test_dataset, EPOCHS, N_CRITIC)"
   ]
  },
  {
   "source": [
    "## Results\n",
    "???"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}