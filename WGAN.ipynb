{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Wasserstein GAN\n",
    "## For generation of features based on Herwig dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0\nPhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import h5py as h5\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras.backend as K\n",
    "import gc\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    \"herwig\": \"GAN-data\\events_anomalydetection_DelphesHerwig_qcd_features.h5\",\n",
    "    \"pythiabg\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_qcd_features.h5\",\n",
    "    \"pythiasig\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_Wprime_features.h5\"\n",
    "}\n",
    "\n",
    "datatypes = [\"herwig\", \"pythiabg\", \"pythiasig\"]\n",
    "features = [\"px\", \"py\", \"pz\", \"m\", \"tau1\", \"tau2\", \"tau3\"]\n",
    "\n",
    "train_features = [\"tau21j2\"] # Can be flexibly changed to suit GAN needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datatype, stop = None):\n",
    "    input_frame = pd.read_hdf(filenames[datatype], stop = stop)\n",
    "    output_frame = input_frame.copy()\n",
    "    for feature in features:\n",
    "        output_frame[feature + \"j1\"] = (input_frame[\"mj1\"] >= input_frame[\"mj2\"])*input_frame[feature + \"j1\"] + (input_frame[\"mj1\"] < input_frame[\"mj2\"])*input_frame[feature + \"j2\"]\n",
    "        output_frame[feature + \"j2\"] = (input_frame[\"mj1\"] >= input_frame[\"mj2\"])*input_frame[feature + \"j2\"] + (input_frame[\"mj1\"] < input_frame[\"mj2\"])*input_frame[feature + \"j1\"]\n",
    "    del input_frame\n",
    "    gc.collect()\n",
    "    output_frame[\"ej1\"] = np.sqrt(output_frame[\"mj1\"]**2 + output_frame[\"pxj1\"]**2 + output_frame[\"pyj1\"]**2 + output_frame[\"pzj1\"]**2)\n",
    "    output_frame[\"ej2\"] = np.sqrt(output_frame[\"mj2\"]**2 + output_frame[\"pxj2\"]**2 + output_frame[\"pyj2\"]**2 + output_frame[\"pzj2\"]**2)\n",
    "    output_frame[\"ejj\"] = output_frame[\"ej1\"] + output_frame[\"ej2\"]\n",
    "    output_frame[\"pjj\"] = np.sqrt((output_frame[\"pxj1\"] + output_frame[\"pxj2\"])**2 + (output_frame[\"pyj1\"] + output_frame[\"pyj2\"])**2 + (output_frame[\"pyj1\"] + output_frame[\"pyj2\"])**2)\n",
    "    output_frame[\"mjj\"] = np.sqrt(output_frame[\"ejj\"]**2 - output_frame[\"pjj\"]**2)\n",
    "    output_frame[\"tau21j1\"] = output_frame[\"tau2j1\"] / output_frame[\"tau1j1\"]\n",
    "    output_frame[\"tau32j1\"] = output_frame[\"tau3j1\"] / output_frame[\"tau2j1\"]\n",
    "    output_frame[\"tau21j2\"] = output_frame[\"tau2j2\"] / output_frame[\"tau1j2\"]\n",
    "    output_frame[\"tau32j2\"] = output_frame[\"tau3j2\"] / output_frame[\"tau2j2\"]\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 0.00005\n",
    "N_CRITIC = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"herwig\")\n",
    "\n",
    "# Ensures all batches have same size\n",
    "\n",
    "df.drop([i for i in range(df.shape[0] % (BATCH_SIZE * 4))], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset consists of 2928 batches of 256 samples each, total 749568 samples\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(df[train_features], test_size = 0.25)\n",
    "len_dataset = int(X_train.shape[0] / BATCH_SIZE)\n",
    "print(\"Dataset consists of {} batches of {} samples each, total {} samples\".format(len_dataset, BATCH_SIZE, len_dataset * BATCH_SIZE))\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(np.array(X_train)).batch(BATCH_SIZE)\n",
    "test_dataset = tf.convert_to_tensor(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(50, input_shape=(len(train_features),)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Dense(len(train_features)))\n",
    "    assert model.output_shape == (None, len(train_features))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(50, input_shape=(len(train_features),)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(50))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(1)) # WGAN: No sigmoid activation in last layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 50)                100       \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                2550      \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 50)                2550      \n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 5,251\nTrainable params: 5,251\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_4 (Dense)              (None, 50)                100       \n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndropout (Dropout)            (None, 50)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 50)                2550      \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 50)                0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 2,701\nTrainable params: 2,701\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    return tf.math.subtract(fake_output, real_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return tf.math.negative(fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.RMSprop(lr=LEARNING_RATE)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(lr=LEARNING_RATE, clipnorm = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to float for loss function plotting\n",
    "def K_eval(x):\n",
    "    try:\n",
    "        return K.get_value(K.to_dense(x))\n",
    "    except:\n",
    "        eval_fn = K.function([], [x])\n",
    "        return eval_fn([])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_generator(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  \n",
    "  return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_discriminator(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, len(train_features)])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "\n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "  \n",
    "  return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_generator(images):\n",
    "    noise = tf.random.normal([X_test.shape[0], len(train_features)])\n",
    "    generated_images = generator(noise, training=True)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_discriminator(images):\n",
    "    noise = tf.random.normal([X_test.shape[0], len(train_features)])\n",
    "    generated_images = generator(noise, training=True)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_gan(generator):\n",
    "    fakedata = np.array(generator(tf.random.normal((100000, len(train_features))), training=False))\n",
    "    plt.title(\"N-subjettiness ratio\")\n",
    "    plt.ylabel(\"Normalized to Unity\")\n",
    "    plt.xlabel(\"$\\\\tau_{21J_2}$\")\n",
    "    plt.hist(np.array(X_train)[:,0], bins = 25, range = (0, 1), color = \"tab:orange\", alpha = 0.5, label = \"Herwig Background\", density = True)\n",
    "    plt.hist(fakedata, bins = 25, range = (0, 1), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, n_critic = 5):\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    print_losses = ((epoch + 1) % 10 == 0)\n",
    "    draw_outputs = ((epoch + 1) % 100 == 0)\n",
    "\n",
    "    train_gen_loss = 0\n",
    "    train_disc_loss = 0\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_gen_loss += K_eval(tf.math.reduce_mean(train_step_generator(image_batch)))\n",
    "      for n in range(n_critic):\n",
    "        train_disc_loss += K_eval(tf.math.reduce_mean(train_step_discriminator(image_batch)))\n",
    "    \n",
    "    train_gen_losses.append(train_gen_loss / len_dataset)\n",
    "    train_disc_losses.append(train_disc_loss / len_dataset / n_critic)\n",
    "\n",
    "    test_gen_losses.append(K_eval(tf.math.reduce_mean(evaluate_generator(test_dataset))))\n",
    "    test_disc_losses.append(K_eval(tf.math.reduce_mean(evaluate_discriminator(test_dataset))))\n",
    "\n",
    "    if print_losses:\n",
    "      print()\n",
    "      print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "      print()\n",
    "      print(\"Generator training loss: \" + str(train_gen_losses[-1]))\n",
    "      print(\"Discriminator training loss: \" + str(train_disc_losses[-1]))\n",
    "      print()\n",
    "      print(\"Generator validation loss: \" + str(test_gen_losses[-1]))\n",
    "      print(\"Discriminator validation loss: \" + str(test_disc_losses[-1]))\n",
    "\n",
    "    if draw_outputs:\n",
    "      print()\n",
    "      print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "      graph_gan(generator)\n",
    "      graph_genloss()\n",
    "      graph_discloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_genloss():\n",
    "    plt.title(\"Generator Loss\")\n",
    "    plt.ylabel(\"Wasserstein Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.plot(train_gen_losses, 'b', label = \"Training loss\")\n",
    "    plt.plot(test_gen_losses, 'r', label = \"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_discloss():\n",
    "    plt.title(\"Discriminator Loss\")\n",
    "    plt.ylabel(\"Wasserstein Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.plot(train_disc_losses, 'b', label = \"Training loss\")\n",
    "    plt.plot(test_disc_losses, 'r', label = \"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_losses = []\n",
    "train_disc_losses = []\n",
    "test_gen_losses = []\n",
    "test_disc_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS, n_critic = N_CRITIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}